# -*- coding: utf-8 -*-
"""AuthorshipIdentification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11iEXyWDrV5QKimGcESPOS9HIJw8eNEIU

# **Required Libraries**
"""

import os
from glob import glob
import zipfile
import chardet
from collections import Counter
from bs4 import BeautifulSoup
import pandas as pd
import random
import re
from typing import List
import pathlib
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from matplotlib import pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder
from joblib import dump, load
from sklearn.metrics import confusion_matrix

"""# **Preprocessing**"""

zipfile.ZipFile("/content/drive/MyDrive/Authorship_Identification/blogs.zip").extractall()

blogs_ds = "/content/blogs"

def get_all_xml_files_in_folder(blogs_ds):
  return glob(os.path.join(blogs_ds, "*.xml")) 
def load_file(filename):
  with open(filename, 'rb') as inf:
    return inf.read()

filenames = get_all_xml_files_in_folder(blogs_ds)
filename = filenames[0]
contents = load_file(filename)

print(filename)
print(contents)
print(type(contents))
chardet.detect(contents)

limit = 30

encodings = {}
for filename in filenames:
  encodings[filename] = chardet.detect(load_file(filename))
  if limit and len(encodings) >= limit:
    break
most_likely_encodings = {filename: encoding['encoding'] for filename, encoding in encodings.items()}
frequencies = Counter(most_likely_encodings.values())
frequencies.most_common(20)

def extract_posts(filename):
  contents = load_file(filename)
  soup = BeautifulSoup(contents, 'xml')
  posts = [p.contents[0] for p in soup.find_all('post') if len(p.contents)]
  return posts

posts = []
number_read = 0
for filename in filenames:
  try:
    number_read += 1
    if number_read > limit:
      break
    posts = posts + extract_posts(filename)
  except Exception as e:
    print("Error with file: ",{filename})
    raise e
print(posts[0][:50],len(posts))

remove_urlLink_match = re.compile("urlLink")
def postprocess(document):
  document = remove_urlLink_match.sub("", document)
  document = document.strip()
  return document

class Post:
  author_number: int
  gender: str
  age: int
  industry: str
  star_sign: str
  #date: str
  post: str
  def to_dict(self):
    return {key: getattr(self, key) for key in ['author_number', 'gender', 'age', 'industry', 'star_sign', 'post']}
  def load_from_file(filename):
    age, author_number, gender, industry, star_sign = Post.extract_attributes_from_filename(filename)
    with open(filename, 'rb') as inf:
      contents = load_file(filename)
      #print(type(contents))
      soup = BeautifulSoup(contents, 'xml')
      posts = [Post.create_from_attributes(author_number, gender, age, industry, star_sign,postprocess(p.contents[0])) for p in soup.find_all('post') if len(p.contents)]
    return posts
  def extract_attributes_from_filename(filename):
    base_filename = pathlib.Path(filename).name
    author_number, gender, age, industry, star_sign, _ = base_filename.split(".")
    author_number = int(author_number)
    age = int(age)
    return age, author_number, gender, industry, star_sign
  def create_from_attributes(author_number, gender, age, industry, star_sign, post):
    p = Post()
    p.author_number = author_number
    p.gender = gender
    p.age = age
    p.industry = industry
    p.star_sign = star_sign
    #p.date = date
    p.post = post
    return p

y = Post.load_from_file("/content/blogs/801916.female.23.Advertising.Taurus.xml")
d = y[0].to_dict()
print(d)

for filename in get_all_xml_files_in_folder(blogs_ds)[:10]:
  print(filename)

filename_id_pattern = re.compile(r"(\\d{3,})\\..*\\..*\\..*\\..*\\.xml"),

def load_dataset_from_raw(blogs_ds, ids=None):
  all_posts = []
  for filename in get_all_xml_files_in_folder(blogs_ds):
    if ids is None or get_filename_id(filename) in ids:
      current_posts = Post.load_from_file(filename)
      all_posts.extend(current_posts)
  return all_posts

def get_filename_id(filename):
  match = filename_id_pattern.search(filename)
  if match:
    return match.group(1)
  else:
    raise ValueError("Could not find an ID in filename", {filename})

def save_dataset(all_posts, output_file):
  dataset = pd.DataFrame([post.to_dict() for post in all_posts])
  dataset.to_parquet(output_file, compression='gzip')
  return dataset

"""dataset_filename = "blogs_processed.parquet"
all_posts_raw = load_dataset_from_raw(dataset_folder)
print(len(all_posts_raw))
save_dataset(all_posts_raw, dataset_filename)
all_posts = load_dataset(dataset_filename)

# **Classification**
"""

def load_dataset(input_file):
  return pd.read_parquet(input_file)

dataset_filename = "/content/drive/MyDrive/Authorship_Identification/blogs_processed.parquet"
all_posts = load_dataset(dataset_filename)

def get_sampled_authors(dataset, sample_authors):
  id_no = dataset['author_number'].isin(sample_authors)
  return dataset[id_no]

sample = get_sampled_authors(all_posts, [3574878, 2845196, 3444474, 3445677, 828046,4284264, 3498812, 4137740, 3662461, 3363271])

documents = sample['post'].values
authors = sample['author_number'].values
documents_train, documents_test, authors_train, authors_test = train_test_split(documents, authors,shuffle = True)
from sklearn.feature_extraction.text import TfidfVectorizer
preprocessor = TfidfVectorizer(analyzer='char', ngram_range=(2,3))
X_train = preprocessor.fit_transform(documents_train)
X_test = preprocessor.transform(documents_test)

model = SGDClassifier()
model.fit(X_train, authors_train)
authors_predicted = model.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(authors_test, authors_predicted))

confusion_matrix(authors_test, authors_predicted)

def build_pipeline():
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(2,3)), SGDClassifier())
  return model
  
def run_analysis(num_authors, model):
  sample_authors = np.random.choice(all_posts['author_number'].unique(), num_authors)
  sample = get_sampled_authors(all_posts, sample_authors)
  documents = sample['post']
  authors = sample['author_number']
  documents_train, documents_test, authors_train, authors_test = train_test_split(documents, authors,shuffle = True)
  model.fit(documents_train, authors_train)
  authors_predicted = model.predict(documents_test)
  return f1_score(y_pred=authors_predicted, y_true=authors_test, average='weighted'),authors_test,authors_predicted,accuracy_score(authors_test,authors_predicted),model

from sklearn.naive_bayes import MultinomialNB

model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(4,5)), SGDClassifier())
f1s,authors_test,authors_predicted,acc = run_analysis(10, model)

print(f1s,acc)

model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(4,5)), SGDClassifier())
f1s,authors_test,authors_predicted = run_analysis(500, model)
#confusion_matrix(authors_test, authors_predicted)
print(f1s)

model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(4,5)), SGDClassifier())
f1s,authors_test,authors_predicted,acc,model = run_analysis(8000, model)
#confusion_matrix(authors_test, authors_predicted)
print(f1s)

for n_authors in range(10, 200, 20):
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(4,5)), SGDClassifier())
  f1s,authors_test,authors_predicted,acc,model = run_analysis(n_authors, model)
  dump(model, "my_model"+str(n_authors)+".clf")
  cnm = confusion_matrix(authors_test, authors_predicted)
  shdd = pd.DataFrame(cnm)
  shdd.to_excel("mDsCM"+str(n_authors)+".xlsx", index=False)
  print(n_authors,f1s,acc)

scores

"""# **Extra Classfications**"""

def run_analysis_cv(num_authors, model, scorer=None):
  if scorer is None:
    scorer = make_scorer(f1_score, average='macro')
  sample_authors = np.random.choice(all_posts['author_number'].unique(), num_authors)
  sample = get_sampled_authors(all_posts, sample_authors)
  documents = sample['post']
  authors = sample['author_number']
  scores = cross_val_score(model, documents, authors, cv=5, scoring=scorer, n_jobs=5)
  return np.mean(scores)

run_analysis_cv(500, model)

model_types = [SGDClassifier(), SVC(kernel='rbf'), SVC(kernel='linear'), BernoulliNB()]

scores = {}
for model_type in model_types:
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(2,3)), model_type)
  key = str(model_type)
  scores[key] = np.array([np.mean([[n_authors, run_analysis_cv(n_authors, model)] for i in range(3)]) for n_authors in np.arange(10, 50, 10)])

scores

def top_k_score(model, X_test, y_test, k=5):
  y_pred = model.predict_proba(X_test)
  sorted_predictions = np.argsort(y_pred, axis=0)
  top_k_predictions = sorted_predictions[:, -k:]
  y_in_top_k = np.any(top_k_predictions == y_test, axis=0)
  return np.mean(y_in_top_k)

scores = {}
top_k_scorer = make_scorer(top_k_score)
model_types = [SGDClassifier(loss='log'), SVC(kernel='rbf', probability=True), SVC(kernel='linear', probability=True), BernoulliNB()]
for model_type in model_types:
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(2,3)), model_type)
  key = str(model_type)
  scores[key] = np.array([np.mean([[n_authors, run_analysis_cv(n_authors, model, scorer=top_k_score)] for i in range(3)]) for n_authors in np.arange(10, 50, 10)])

model_types = [SGDClassifier(), SVC(kernel='rbf'), SVC(kernel='linear'), BernoulliNB(), GaussianNB()]

num_authors = 8000

sample_authors = np.random.choice(all_posts['author_number'].unique(), num_authors)
sample = get_sampled_authors(all_posts, sample_authors)
documents = sample['post']
authors = sample['author_number']

import cProfile, pstats, io
from pstats import SortKey

def build_pipeline(classifier):
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(2,3)), classifier)
  return model

def run_model(classifier, documents, authors):
  le = LabelEncoder()
  authors = le.fit_transform(authors)
  model = build_pipeline(classifier)
  pr = cProfile.Profile()
  pr.enable()
  cross_val_score(model, documents, authors, cv=5, n_jobs=1)
  pr.disable()
  return pr

profiles = {}
for model_num, model_type in enumerate(model_types):
  print(model_type)
  profiles[model_type] = run_model(model_type, documents, authors)
  filename = f"model_{model_num}.stats"
  print(" ", filename)
  profiles[model_type].dump_stats(filename)
  s = pstats.Stats(filename)
  s.sort_stats('cumulative').print_stats(3)
  print("-" * 80)

X_train, X_test, y_train, y_test = train_test_split(documents, authors)

print(len(X_train),len(documents))

model = build_pipeline(SGDClassifier())
model.fit(X_train, y_train)

authors_predicted = model.predict(X_test)
print(classification_report(authors_test, authors_predicted))

from joblib import dump, load
dump(model, "my_model.clf")

_model = None
def predict_document(document):
  global _model
  if _model is None:
    _model = load("my_model.clf")
  return _model.predict([document])[0]

document = X_test.values[0]

predict_document(document)

"""# **Modified Dataset**"""

occup = sorted(list(all_posts['industry'].unique()))

occup

mod_pos_df = pd.DataFrame()

mod_pos_df

"""shtdf = (all_posts.groupby(['industry']).get_group(('Accounting'))).groupby('author_number')
ind = x.size().sort_values(ascending = False).index[0]
mod_pos_df = mod_pos_df.append(x.get_group(ind),ignore_index=True)
#x.size().sort_values(ascending = False).index[0]"""

for i in occup:
  shtdf = (all_posts.groupby(['industry']).get_group((i))).groupby('author_number')
  ind = shtdf.size().sort_values(ascending = False).index[0]
  mod_pos_df = mod_pos_df.append(shtdf.get_group(ind),ignore_index=True)

all_posts['age'].unique()

mod_pos_df.groupby(['age','author_number']).size()

mod_pos_df.to_parquet('modified_blogs.parquet', compression='gzip')

"""# **Working with modified Dataset**"""

def load_dataset(input_file):
  return pd.read_parquet(input_file)

mod_pos_df = load_dataset('/content/drive/MyDrive/Authorship_Identification/modified_blogs.parquet')

mod_pos_df

def build_pipeline():
  model = make_pipeline(TfidfVectorizer(analyzer='char', ngram_range=(2,3)), SGDClassifier())
  return model
  
def run_analysis(df, model):
  documents = df['post']
  authors = df['author_number']
  documents_train, documents_test, authors_train, authors_test = train_test_split(documents, authors,shuffle = True)
  model.fit(documents_train, authors_train)
  authors_predicted = model.predict(documents_test)
  return f1_score(y_pred=authors_predicted, y_true=authors_test, average='weighted'),documents_test,authors_test,authors_predicted,model

model = build_pipeline()
f1sc,documents_test,authors_test,authors_predicted,model = run_analysis(mod_pos_df,model)

f1sc

accuracy_score(authors_test,authors_predicted)
#confusion_matrix(authors_test, authors_predicted)

from sklearn.metrics import confusion_matrix
cnm = confusion_matrix(authors_test, authors_predicted)
shdd = pd.DataFrame(cnm)
shdd.to_excel("mDsCM.xlsx", index=False)

cnm

shdd = pd.DataFrame(cnm)

shdd.to_excel("mDsCM.xlsx", index=False)

dump(model, "my_model.clf")

_model = load("/content/drive/MyDrive/Authorship_Identification/my_model.clf")

tpost = random.randint(0,len(mod_pos_df))

tpost

mod_pos_df['author_number'][tpost]

_model.predict([mod_pos_df['post'][tpost]])

tpost

mod_pos_df['author_number'][tpost]

_model.predict([mod_pos_df['post'][tpost]])