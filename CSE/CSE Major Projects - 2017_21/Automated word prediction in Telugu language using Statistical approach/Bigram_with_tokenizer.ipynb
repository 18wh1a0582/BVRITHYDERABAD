{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uHRAvwVCjmy0"
   },
   "outputs": [],
   "source": [
    "#importing  packages\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XohYoNZrjmy1",
    "outputId": "dd17c5ac-75f8-4f42-cf35-e4a6a1a1d21c"
   },
   "outputs": [],
   "source": [
    "#!pip install cltk==v0.1.99       #installing cltk\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "tokenizer = TokenizeSentence('telugu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/karthik/Desktop/keeru/datasets/tewiki_all.txt'    \n",
    "with open(path) as myfile:\n",
    "    lines = [line.split('. ') for line in myfile.readlines()]\n",
    "testlines = lines[450000:500000]\n",
    "lines = lines[:450000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7mTQLb6xjmy4"
   },
   "outputs": [],
   "source": [
    "#function removing the html tags,punctuations,numbers,english words\n",
    "def preprocessing_data(str):           \n",
    "    u = re.sub('<[^<]+?>','',str)\n",
    "    o = re.sub('\\u200c',' ',u)\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    translated = o.translate(table)\n",
    "    w = re.sub('[a-zA-Z]*','',translated)\n",
    "    d = re.sub(' +',' ',w)\n",
    "    #x = re.sub('[0-9]*','',d)\n",
    "    y = re.sub(\"\\n\",'',d)\n",
    "    return y\n",
    "\n",
    "#getting a list of sentences that are clean\n",
    "def get_preprocessed_sentences(sentences):    \n",
    "    tokenized_sentences=[]\n",
    "    for i in range(0,len(sentences)):\n",
    "        temp = []\n",
    "        for j in range(0,len(sentences[i])):\n",
    "            final = preprocessing_data(sentences[i][j])\n",
    "            if len(final) != 0:\n",
    "                temp.append(final)\n",
    "        tokenized_sentences.append(temp)\n",
    "    return tokenized_sentences\n",
    "\n",
    "train_preprocessed_sentences = get_preprocessed_sentences(lines)\n",
    "test_preprocessed_sentences = get_preprocessed_sentences(testlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LNzuhs28jmy3"
   },
   "outputs": [],
   "source": [
    "#function to tokenize the sentences\n",
    "def getting_sentences(line):               \n",
    "    sentences = []\n",
    "    for i in range(0,len(line)):\n",
    "        for j in range(0,len(line[i])):\n",
    "            sentence = line[i][j]\n",
    "            s = tokenizer.tokenize(sentence)\n",
    "            sentences.append(s)\n",
    "    return sentences\n",
    "train_sentences = getting_sentences(train_preprocessed_sentences)\n",
    "test_sentences = getting_sentences(test_preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91IbY-gijmy4",
    "outputId": "e0c716b3-b1bf-4d3a-b03b-c2f28715f630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s> ', 'మొదటి', 'పేజీ', ' </s>'], ['<s> ', 'గుంటూరు', 'జిల్లా', ' </s>'], ['<s> ', 'గుంటూరు', 'జిల్లా', '11391', 'చకిమీ', ' </s>'], ['<s> ', 'ల', 'విస్తీర్ణములో', 'వ్యాపించి', '4889230', '2011', 'గణన', 'జనాభా', 'కలిగిఉన్నది', ' </s>'], ['<s> ', 'ఆగ్నేయాన', 'బంగాళాఖాతము', 'దక్షిణాన', 'ప్రకాశం', 'జిల్లా', 'పశ్చిమాన', 'మహబూబ్', 'నగర్', 'జిల్లా', 'మరియు', 'వాయువ్యాన', 'నల్గొండ', 'జిల్లా', 'సరిహద్దులుగా', 'ఉన్నాయి', ' </s>'], ['<s> ', 'దీని', 'ముఖ్యపట్టణం', 'గుంటూరు', ' </s>'], ['<s> ', 'ఈ', 'జిల్లాకు', 'అతి', 'పురాతన', 'చరిత్ర', 'ఉంది', ' </s>'], ['<s> ', 'మౌర్యులు', 'శాతవాహనులు', 'పల్లవులు', 'చాళుక్యులు', 'కాకతీయులు', 'రెడ్డి', 'రాజులు', 'విజయనగర', 'రాజులు', 'పరిపాలించారు', ' </s>'], ['<s> ', 'పల్నాటి', 'యుద్ధం', 'ఇక్కడే', 'జరిగింది', ' </s>'], ['<s> ', 'మొగలు', 'సామ్రాజ్యం', 'నిజాం', 'పాలన', 'ఈస్ట్', 'ఇండియా', 'కంపెనీ', 'ఆ', 'తరువాత', 'మద్రాసు', 'ప్రసిడెన్సీలో', 'భాగమైనది', ' </s>']]\n",
      "[['<s> ', 'గ్రామంలో', 'కుళాయిల', 'ద్వారా', 'రక్షిత', 'మంచినీటి', 'సరఫరా', 'జరుగుతోంది', ' </s>'], ['<s> ', 'కుళాయిల', 'ద్వారా', 'శుద్ధి', 'చేయని', 'నీరు', 'కూడా', 'సరఫరా', 'అవుతోంది', ' </s>'], ['<s> ', 'గ్రామంలో', 'ఏడాది', 'పొడుగునా', 'చేతిపంపుల', 'ద్వారా', 'నీరు', 'అందుతుంది', ' </s>'], ['<s> ', 'మురుగునీరు', 'బహిరంగ', 'కాలువల', 'ద్వారా', 'ప్రవహిస్తుంది', ' </s>'], ['<s> ', 'మురుగునీరు', 'బహిరంగంగా', 'కచ్చా', 'కాలువల', 'ద్వారా', 'కూడా', 'ప్రవహిస్తుంది', ' </s>'], ['<s> ', 'మురుగునీటిని', 'శుద్ధి', 'ప్లాంట్', 'లోకి', 'పంపిస్తున్నారు', ' </s>'], ['<s> ', 'గ్రామంలో', 'సంపూర్ణ', 'పారిశుధ్య', 'పథకం', 'అమలవుతోంది', ' </s>'], ['<s> ', 'సామాజిక', 'మరుగుదొడ్డి', 'సౌకర్యం', 'లేదు', ' </s>'], ['<s> ', 'ఇంటింటికీ', 'తిరిగి', 'వ్యర్థాలను', 'సేకరించే', 'వ్యవస్థ', 'లేదు', ' </s>'], ['<s> ', 'సామాజిక', 'బయోగ్యాస్', 'ఉత్పాదక', 'వ్యవస్థ', 'లేదు', ' </s>']]\n"
     ]
    }
   ],
   "source": [
    "# adding start and end delimiter, then splitting\n",
    "def delimiting_spliting(tokenized_sentences):   \n",
    "    tokenized_sentences = [x for x in tokenized_sentences if x != ['\\n'] and x != [' \\n']]\n",
    "    sentences_with_delimiter = []\n",
    "    for i in range(0,len(tokenized_sentences)):\n",
    "        s = tokenized_sentences[i]\n",
    "        if len(s) == 0:\n",
    "            continue\n",
    "        temp = ['<s> ']+s+[' </s>']                #adding start and end delimiter\n",
    "        sentences_with_delimiter.append(temp)\n",
    "    print(sentences_with_delimiter[:10])\n",
    "    #split_data =[]\n",
    "    #for i in range(0,len(sentences_with_delimiter)):\n",
    "        #d = sentences_with_delimiter[i].split()\n",
    "        #split_data.append(d)                #spliting the words, and getting the list that contain list of words of every sentence\n",
    "    return sentences_with_delimiter\n",
    "split_train_data = delimiting_spliting(train_sentences)\n",
    "split_test_data = delimiting_spliting(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LR_CnMqXjmy5"
   },
   "outputs": [],
   "source": [
    "#getting a single list of words\n",
    "def getting_words_list(split_data):      \n",
    "    list = [item for sublist in split_data for item in sublist]\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Q6byS1c6jmy7"
   },
   "outputs": [],
   "source": [
    "words_train = getting_words_list(split_train_data)   #train data words\n",
    "words_test = getting_words_list(split_test_data)     #test data words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F2EWRTsCjmy8"
   },
   "outputs": [],
   "source": [
    "unique_words = len(set(words_train)) # getting number of unique words in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sOecqMW1jmy9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#geting the dictionary(model) from the train data, where keys are the history words and values are \n",
    "#another dictionary which contains keys as the next word and its values corresponding to the number \n",
    "#of times that words has occurred\n",
    "\n",
    "dic = {}\n",
    "for i in range(0,len(words_train)-1):\n",
    "    t = words_train[i]\n",
    "    s = dic.get(t)\n",
    "    next = i+1\n",
    "    if s == None:\n",
    "        dic[t] = [words_train[next]]\n",
    "    else:\n",
    "        s.append(words_train[next])\n",
    "        temp = {t:s}\n",
    "        dic.update(temp)\n",
    "#test_dic is a dictionary where keys are the history words of the test data and values is the list of next words     \n",
    "\n",
    "test_dic = {}\n",
    "for i in range(0,len(words_test)-1):\n",
    "    t = words_test[i]\n",
    "    s = test_dic.get(t)\n",
    "    next = i+1\n",
    "    if s == None:\n",
    "        test_dic[t] = [words_test[next]]\n",
    "    else:\n",
    "        s.append(words_test[next])\n",
    "        temp = {t:s}\n",
    "        test_dic.update(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YrKIPVnxjmy9"
   },
   "outputs": [],
   "source": [
    "# getting the count of each element in the list\n",
    "def frequency_count(list):\n",
    "    c = collections.Counter(list)\n",
    "    count_dic={}\n",
    "    for i in range(0,len(list)):\n",
    "        count_dic[list[i]] = c[list[i]]\n",
    "    return count_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6eWUtGPqjmy-"
   },
   "outputs": [],
   "source": [
    "# updating the dic with the word frequency\n",
    "for word in dic:\n",
    "    s = dic.get(word)\n",
    "    value = frequency_count(s)\n",
    "    temporary = {word:value}\n",
    "    dic.update(temporary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sNSZn-7Rjmy-"
   },
   "outputs": [],
   "source": [
    "# getting the probability of each predicted word\n",
    "def get_probability(s1,s2):\n",
    "    firstWord = s1\n",
    "    nextWord = s2\n",
    "    inner_dic = dic.get(firstWord)\n",
    "    \n",
    "    if inner_dic == None:\n",
    "        return 0\n",
    "    else:\n",
    "        numerator = inner_dic.get(nextWord)\n",
    "        if numerator == None:\n",
    "            return 0\n",
    "        else:\n",
    "            denominator = sum(inner_dic.values())\n",
    "            return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QcXYd10Vjmy-"
   },
   "outputs": [],
   "source": [
    "# getting the add-one probability of each predicted word\n",
    "def get_addOne_probability(s1,s2):\n",
    "    firstWord = s1\n",
    "    nextWord = s2\n",
    "    inner_dic = dic.get(firstWord)\n",
    "    if inner_dic == None:\n",
    "        return 0 \n",
    "    else:\n",
    "        numerator = inner_dic.get(nextWord)\n",
    "        if numerator == None:\n",
    "            numerator = 1\n",
    "        else:\n",
    "            numerator = numerator + 1\n",
    "        denominator = sum(inner_dic.values()) + unique_words\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4w2_GHRvjmy_"
   },
   "outputs": [],
   "source": [
    "# getting the add-alpha probability of each predicted word\n",
    "def get_addAlpha_probability(s1,s2):\n",
    "    firstWord = s1\n",
    "    nextWord = s2\n",
    "    alpha = 0.1\n",
    "    inner_dic = dic.get(firstWord)\n",
    "    if inner_dic == None:\n",
    "        return 0 \n",
    "    else:\n",
    "        numerator = inner_dic.get(nextWord)\n",
    "        if numerator == None:\n",
    "            numerator = alpha\n",
    "        else:\n",
    "            numerator = numerator + alpha\n",
    "        denominator = sum(inner_dic.values()) + (unique_words*alpha)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aCcQwMrGjmy_"
   },
   "outputs": [],
   "source": [
    "#preparing probability dictionaries with corresponding probabilities\n",
    "prob_dic = {}\n",
    "addOneProb_dic = {}\n",
    "addAlphaProb_dic = {}\n",
    "for i in range(0,len(words_test)-2):\n",
    "    bigram = words_test[i]+words_test[i+1]\n",
    "    prob = get_probability(words_test[i],words_test[i+1])\n",
    "    addOneProb = get_addOne_probability(words_test[i],words_test[i+1])\n",
    "    prob_dic[bigram]=prob\n",
    "    addOneProb_dic[bigram] = addOneProb\n",
    "    addAlphaProb = get_addAlpha_probability(words_test[i],words_test[i+1])\n",
    "    addAlphaProb_dic[bigram] = addAlphaProb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CXZ9Mhk8jmzA"
   },
   "outputs": [],
   "source": [
    "# computing perplexity Score\n",
    "def perplexityScore(prob_dic):\n",
    "    logProb = 0\n",
    "    num = 0\n",
    "    for word in prob_dic:\n",
    "        p = prob_dic.get(word)\n",
    "        if p == 0:\n",
    "            logProb = logProb\n",
    "        else:\n",
    "            w = m.log(p**-1,10)\n",
    "            logProb = logProb + w \n",
    "            num = num + 1\n",
    "    return (2**(logProb/num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoxrCp1gjmzA",
    "outputId": "aa78fea7-d83d-438c-bd00-ef30f4139b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.275553930100999\n",
      "33.564570220418624\n",
      "28.40261011567869\n"
     ]
    }
   ],
   "source": [
    "# perplexity Scores for different cases\n",
    "ans = perplexityScore(prob_dic)\n",
    "print(ans)\n",
    "ans1 = perplexityScore(addOneProb_dic)\n",
    "print(ans1)\n",
    "ans2 = perplexityScore(addAlphaProb_dic)\n",
    "print(ans2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab0ROwlSjmzB",
    "outputId": "ec38e418-39db-4a82-9512-c1d21a7ed800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the 1st word మహబూబ్\n",
      "మహబూబ్\n",
      "నగర్\n"
     ]
    }
   ],
   "source": [
    "#input interface\n",
    "s1 = input(\"Enter the 1st word \" )\n",
    "input_word = s1\n",
    "print(input_word)\n",
    "def getting_2nd(input_word):\n",
    "    s = dic.get(input_word)\n",
    "    if s == None:\n",
    "        print(\"not found\")\n",
    "        return\n",
    "    else:\n",
    "        maximum = max(s, key=s.get)\n",
    "        return maximum\n",
    "print(getting_2nd(input_word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvjUzN-ejmzB",
    "outputId": "03a14555-a7f3-4a4c-a2a7-f232ef75459f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the 1st word గుంటూరు\n",
      "జిల్లా\n",
      "రహదారి\n",
      "గ్రామం\n",
      "నుండి\n",
      "10\n",
      "కిమీ\n",
      " </s>\n",
      "<s> \n",
      "గ్రామంలో\n",
      "మగవారి\n",
      "సంఖ్య\n",
      "0\n",
      " </s>\n",
      "<s> \n"
     ]
    }
   ],
   "source": [
    "#predicting a sequence of words\n",
    "s1 = input(\"Enter the 1st word \" )\n",
    "input_word = s1\n",
    "i = 1\n",
    "next = getting_2nd(input_word)\n",
    "for i in range(1,15):\n",
    "        print(next)\n",
    "        next2 = getting_2nd(next)\n",
    "        next = next2\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "గుంటూరు\n",
    "సరిహద్దులుగా\n",
    "మహబూబ్"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "updatedBigram_with(working).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
